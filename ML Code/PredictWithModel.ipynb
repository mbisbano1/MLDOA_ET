{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the neccesary libraries needed\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "import numpy as np\n",
    "from numpy import insert\n",
    "import os\n",
    "import time\n",
    "\n",
    "# keras tuner for hyperparameter tuning\n",
    "import keras_tuner as kt\n",
    "\n",
    "# tensorflow Libraries\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import sparse_categorical_crossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise Reduction Dataset\n",
    "train = pd.read_csv('D:\\\\OneDrive\\\\OneDrive - University of Massachusetts Dartmouth\\\\ECE457_Senior_Design_ECE5\\\\CSV_Files\\\\0001_1404.038_port_FinalCleaned_RollReady.csv')\n",
    "train2 = pd.read_csv('D:\\\\OneDrive\\\\OneDrive - University of Massachusetts Dartmouth\\\\ECE457_Senior_Design_ECE5\\\\CSV_Files\\\\0001_1404.002_port_FinalCleaned_RollReady.csv')\n",
    "test = pd.read_csv('D:\\\\OneDrive\\\\OneDrive - University of Massachusetts Dartmouth\\\\ECE457_Senior_Design_ECE5\\\\CSV_Files\\\\0001_1404.038_stbd_FinalCleaned_RollReady.csv')\n",
    "test2 = pd.read_csv('D:\\\\OneDrive\\\\OneDrive - University of Massachusetts Dartmouth\\\\ECE457_Senior_Design_ECE5\\\\CSV_Files\\\\0001_1404.002_stbd_FinalCleaned_RollReady.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up the features and labels from both the training and testing datasets\n",
    "#x_train=train.iloc[:,4:24]\n",
    "\n",
    "#x_train=train.iloc[:,3:24] # sample time delay through Q10\n",
    "\n",
    "x_train1=train.iloc[:,3:25] # sample time delay through roll\n",
    "x_train2 = train2.iloc[:, 3:25]\n",
    "#x_train=train38.iloc[:,3:24]\n",
    "\n",
    "x_train1['Amplitude']=train.iloc[:,29]\n",
    "x_train2['Amplitude']=train2.iloc[:,29]\n",
    "#x_train['Amplitude']=train38.iloc[:,29]\n",
    "\n",
    "\n",
    "#x_test=test.iloc[:,4:24]\n",
    "#x_test=test.iloc[:,3:24]    # sample time delay through Q10\n",
    "\n",
    "x_test=test.iloc[:,3:25]    # sample time delay through roll\n",
    "x_test2=test2.iloc[:,3:25]    # sample time delay through roll\n",
    "#x_test['Roll'] = x_test['Roll']\n",
    "\n",
    "\n",
    "x_test['Amplitude']=test.iloc[:,29]\n",
    "x_test2['Amplitude']=test2.iloc[:,29]\n",
    "#x_test['Amplitude']=test38.iloc[:,29]\n",
    "\n",
    "y_train1=train.iloc[:,26]\n",
    "y_train2=train2.iloc[:,26]\n",
    "#y_train=train38.iloc[:,26]\n",
    "\n",
    "y_test=test.iloc[:,26]\n",
    "y_test2=test2.iloc[:,26]\n",
    "#y_test=test38.iloc[:,26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xframes = [x_train1, x_train2]\n",
    "x_train = pd.concat(xframes, sort=False)\n",
    "yframes = [y_train1, y_train2]\n",
    "y_train = pd.concat(yframes, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7622237\n",
      "7622237\n"
     ]
    }
   ],
   "source": [
    "# Scale the Features and Labels from [-1,1]\n",
    "\n",
    "s1=MinMaxScaler(feature_range=(0,1))\n",
    "#s1=MinMaxScaler(feature_range=(0,1))\n",
    "x_train_scale=s1.fit_transform(x_train)\n",
    "\n",
    "#s2=MinMaxScaler(feature_range=(0,1))\n",
    "#s2=MinMaxScaler(feature_range=(0,1))\n",
    "x_test_scale=s1.transform(x_test)\n",
    "x_test_scale=x_test_scale\n",
    "x_test_scale[:, 21] = -x_test_scale[:, 21]\n",
    "x_test2_scale=s1.transform(x_test2)\n",
    "x_test2_scale[:, 21] = -x_test2_scale[:, 21]\n",
    "\n",
    "# Scale DOA's from (0 to 1)\n",
    "#s3=MinMaxScaler(feature_range=(0,1))\n",
    "#y_train_scale = s3.fit_transform(train[['DOA']])\n",
    "y_train_scale= y_train.to_numpy()\n",
    "\n",
    "#s4=MinMaxScaler(feature_range=(0,1))\n",
    "#y_test_scale = s4.fit_transform(test[['DOA']])\n",
    "y_test_scale = y_test.to_numpy()\n",
    "y_test2_scale = y_test2.to_numpy()\n",
    "print(len(x_train_scale))\n",
    "print(len(y_train_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7622237\n",
      "3382112\n",
      "[ 0.00000000e+00 -3.05400003e+00 -5.40000005e-02 ...  8.02140007e+01\n",
      "  8.02440007e+01  8.02680007e+01]\n",
      "7622237\n",
      "[ 0.          3.69000003  2.39400002 ... 76.59600067 76.35000066\n",
      " 76.32600066]\n",
      "3382112\n"
     ]
    }
   ],
   "source": [
    "# Shift the DOA to the left by one to use the current DOA label with the current I and Q data.\n",
    "# Normally the TimeseriesGenerator function uses past values to predict the future but we would like current data to help predict the current samples' DOA\n",
    "print(len(y_train_scale))\n",
    "print(len(y_test_scale))\n",
    "y_train_scale=insert(y_train_scale, 0, 0)\n",
    "y_train_scale=np.delete(y_train_scale, -1)\n",
    "y_test_scale=insert(y_test_scale, 0,0)\n",
    "y_test_scale=np.delete(y_test_scale, -1)\n",
    "y_test2_scale=insert(y_test2_scale, 0,0)\n",
    "y_test2_scale=np.delete(y_test2_scale, -1)\n",
    "print(y_train_scale)\n",
    "print(len(y_train_scale))\n",
    "print(y_test_scale)\n",
    "print(len(y_test_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7622237"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the windows neccesary for the LSTM model within tensorflow keras\n",
    "# Below are the arguments of the timeseries_dataset_from_array function\n",
    "# The function takes a numpy array and makes a timeseries out of it\n",
    "NumSampsPerPing= 4301-24+1\n",
    "data=x_train_scale # the data to make the windows\n",
    "targets=y_train_scale # time steps in the data (don't need)\n",
    "sequence_length=5 # window length\n",
    "sequence_stride=1# period between successive output sequences\n",
    "sampling_rate=1 # period between successive individual timesteps     within sequences\n",
    "batch_size=32 # number of time series samples in each batch\n",
    "shuffle=False #shuffle the data before making the windows\n",
    "seed=None # is related to shuffle\n",
    "start_index=None # is related to shuffle\n",
    "end_index=None # is related to shuffle\n",
    "n_features=23\n",
    "\n",
    "test_data=x_test_scale\n",
    "test_targets=y_test_scale\n",
    "test2_data = x_test2_scale\n",
    "test2_targets = y_test2_scale\n",
    "dataRows = data.shape[0]\n",
    "dataRows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7422237 , 7622237\n"
     ]
    }
   ],
   "source": [
    "#validationStartRow = dataRows - NumSampsPerPing*281\n",
    "validationStartRow = dataRows - 200*1000\n",
    "#validationStartRow = dataRows - 8*1000\n",
    "\n",
    "print(validationStartRow, ',',dataRows)\n",
    "\n",
    "train_data = data[:validationStartRow-1]\n",
    "#train_data = data\n",
    "#   train_data.shape[0]\n",
    "train_targets = targets[:validationStartRow-1]\n",
    "#train_targets = targets\n",
    "#   train_targets.shape[0]\n",
    "val_data = data[validationStartRow:]\n",
    "#   val_data.shape[0], dataRows-validationStartRow\n",
    "y_val_scale = y_train_scale[validationStartRow:]\n",
    "\n",
    "val_targets = targets[validationStartRow:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#inputs=TimeseriesGenerator(data, targets,sequence_length, batch_size)\n",
    "train_inputs=TimeseriesGenerator(data=train_data,targets=train_targets,length=sequence_length,sampling_rate=sampling_rate,stride=sequence_stride,batch_size=batch_size, shuffle=True)\n",
    "trainentire_inputs = TimeseriesGenerator(data=data,targets=targets,length=sequence_length,sampling_rate=sampling_rate,stride=sequence_stride,batch_size=256)\n",
    "\n",
    "val_inputs = TimeseriesGenerator(data=val_data,targets=val_targets,length=sequence_length,sampling_rate=sampling_rate,stride=sequence_stride,batch_size=256, shuffle=False)\n",
    "\n",
    "#test_inputs=TimeseriesGenerator(test_data[0:40000], -1*test_targets[0:40000],sequence_length, batch_size)\n",
    "test_inputs=TimeseriesGenerator(data=test_data,targets=test_targets,length=sequence_length,sampling_rate=sampling_rate,stride=sequence_stride,batch_size=256)\n",
    "test_inputs2=TimeseriesGenerator(data=test2_data,targets=test2_targets,length=sequence_length,sampling_rate=sampling_rate,stride=sequence_stride,batch_size=256)\n",
    "\n",
    "#inputs=tf.keras.preprocessing.timeseries_dataset_from_array(data, targets, sequence_length, sequence_stride, sampling_rate, batch_size, shuffle)\n",
    "#print(type(inputs))\n",
    "\n",
    "#print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at D:\\OneDrive\\OneDrive - University of Massachusetts Dartmouth\\ECE457_Senior_Design_ECE5\\SavedModels\\MB\\Mar10_2022\\model_row31.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21352/2034136081.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#model = keras.models.load_model('model/')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#model = keras.models.load_model('model_3_13_22_epoch4/model/')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"D:\\\\OneDrive\\\\OneDrive - University of Massachusetts Dartmouth\\\\ECE457_Senior_Design_ECE5\\\\SavedModels\\\\MB\\\\Mar10_2022\\\\model_row31.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\keras_tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\keras_tf\\lib\\site-packages\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'No file or directory found at {filepath}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_hdf5_filepath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mh5py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             raise ImportError(\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at D:\\OneDrive\\OneDrive - University of Massachusetts Dartmouth\\ECE457_Senior_Design_ECE5\\SavedModels\\MB\\Mar10_2022\\model_row31.h5"
     ]
    }
   ],
   "source": [
    "#model = keras.models.load_model('model/')\n",
    "#model = keras.models.load_model('model_3_13_22_epoch4/model/')\n",
    "model = keras.models.load_model(filepath=\"D:\\\\OneDrive\\\\OneDrive - University of Massachusetts Dartmouth\\\\ECE457_Senior_Design_ECE5\\\\SavedModels\\\\MB\\\\Mar10_2022\\\\model_row31.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainpredicted=model.predict(train_inputs, verbose=1)\n",
    "#valpredicted=model.predict(val_inputs, verbose=1)\n",
    "#trainentirepredicted=model.predict(trainentire_inputs, verbose=1)\n",
    "predicted038=model.predict(test_inputs, verbose=1)\n",
    "predicted002=model.predict(test_inputs2, verbose=1)\n",
    "#trainpredicted.shape, valpredicted.shape, predicted.shape, trainentirepredicted.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "674a9451ed4de84c8c52b62b7b95bc1f8984ddc9599597b49394de9958455427"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('keras_tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
