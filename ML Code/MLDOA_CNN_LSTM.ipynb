{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the neccesary libraries needed\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "import numpy as np\n",
    "from numpy import insert\n",
    "import os\n",
    "import time\n",
    "\n",
    "# keras tuner for hyperparameter tuning\n",
    "import keras_tuner as kt\n",
    "\n",
    "# tensorflow Libraries\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, LSTM, Conv2D, MaxPooling2D, Flatten, TimeDistributed, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import sparse_categorical_crossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[10 10 21]\n",
    "#[1920 1080 3]\n",
    "#[255 0 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.test.gpu_device_name())\n",
    "print(tf.config.experimental.list_physical_devices(device_type='GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the CSV Training and Testing Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the csv's (IMPORTANT: You need at least 16 Gb of RAM to proceed)\n",
    "#train=pd.read_csv(r'C:\\Users\\Daniel\\OneDrive - University of Massachusetts Dartmouth\\ECE457_Senior_Design_ECE5\\CSV_Files\\PortTraining_1404_002_FINAL.csv')\n",
    "#test=pd.read_csv(r'C:\\Users\\Daniel\\OneDrive - University of Massachusetts Dartmouth\\ECE457_Senior_Design_ECE5\\CSV_Files\\StbdTesting_1404_002_FINAL.csv')\n",
    "train = pd.read_csv('D:\\\\OneDrive\\\\OneDrive - University of Massachusetts Dartmouth\\\\ECE457_Senior_Design_ECE5\\\\CSV_Files\\\\PortTraining_1404_002_FINAL_startingAt24.csv')\n",
    "test = pd.read_csv('D:\\\\OneDrive\\\\OneDrive - University of Massachusetts Dartmouth\\\\ECE457_Senior_Design_ECE5\\\\CSV_Files\\\\StbdTesting_1404_002_FINAL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split up the Features and Labels from both the Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up the features and labels from both the training and testing datasets\n",
    "x_train=train.iloc[:,4:24]\n",
    "#x_train['TWTT']=train.iloc[:,28]  I feel as though the TWTT has NO impact on the DOA so we should NOT include it in training\n",
    "x_train['Amplitude']=train.iloc[:,29]\n",
    "\n",
    "\n",
    "x_test=test.iloc[:,4:24]\n",
    "#x_test['TWTT']=test.iloc[:,28] I feel as though the TWTT has NO impact on the DOA so we should NOT include it in training\n",
    "x_test['Amplitude']=test.iloc[:,29]\n",
    "\n",
    "y_train=train.iloc[:,26]\n",
    "\n",
    "y_test=test.iloc[:,26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the Features from -1 to 1 so that Training is Easier for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the Features and Labels from [-1,1]\n",
    "s1=MinMaxScaler(feature_range=(0,1))\n",
    "#s1=MinMaxScaler(feature_range=(-1,1))\n",
    "x_train_scale=s1.fit_transform(x_train)\n",
    "\n",
    "#s2=MinMaxScaler(feature_range=(-1,1))\n",
    "s2=MinMaxScaler(feature_range=(0,1))\n",
    "x_test_scale=s2.fit_transform(x_test)\n",
    "x_test_scale=x_test_scale\n",
    "\n",
    "# Scale DOA's from (0 to 1)\n",
    "s3=MinMaxScaler(feature_range=(0,1))\n",
    "#y_train_scale = s3.fit_transform(train[['DOA']])\n",
    "y_train_scale= y_train.to_numpy()\n",
    "\n",
    "s4=MinMaxScaler(feature_range=(0,1))\n",
    "#y_test_scale = s4.fit_transform(test[['DOA']])\n",
    "y_test_scale= y_test.to_numpy()\n",
    "\n",
    "print(len(x_train_scale))\n",
    "print(len(y_train_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_DOA_df= pd.DataFrame(y_test_scale, columns=['TEST DOAs'])\n",
    "Test_DOA_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_DOA_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame(x_train_scale, columns= ['I1', 'Q1', 'I2', 'Q2', 'I3', 'Q3', 'I4', 'Q4', 'I5', 'Q5', 'I6', 'Q6', 'I7', 'Q7', 'I8', 'Q8', 'I9', 'Q9', 'I10', 'Q10', 'Amplitude'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shift the DOA's to the Left by one so that the Current I and Q data Align with the current DOA while using TimeSeriesGenerator to Window the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the DOA to the left by one to use the current DOA label with the current I and Q data.\n",
    "# Normally the TimeseriesGenerator function uses past values to predict the future but we would like current data to help predict the current samples' DOA\n",
    "print(len(y_train_scale))\n",
    "print(len(y_test_scale))\n",
    "y_train_scale=insert(y_train_scale, 0, 0)\n",
    "y_train_scale=np.delete(y_train_scale, -1)\n",
    "y_test_scale=insert(y_test_scale, 0,0)\n",
    "y_test_scale=np.delete(y_test_scale, -1)\n",
    "print(y_train_scale)\n",
    "print(len(y_train_scale))\n",
    "print(y_test_scale)\n",
    "print(len(y_test_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Windows Neccesary for the LSTM model using the TimeSeriesGenerator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the windows neccesary for the LSTM model within tensorflow keras\n",
    "# Below are the arguments of the timeseries_dataset_from_array function\n",
    "# The function takes a numpy array and makes a timeseries out of it\n",
    "NumSampsPerPing= 4301-24+1\n",
    "data=x_train_scale # the data to make the windows\n",
    "targets=y_train_scale # time steps in the data (don't need)\n",
    "sequence_length=10 # window length\n",
    "sequence_stride=1# period between successive output sequences\n",
    "sampling_rate=1 # period between successive individual timesteps     within sequences\n",
    "batch_size=NumSampsPerPing # number of time series samples in each batch\n",
    "shuffle=False #shuffle the data before making the windows\n",
    "seed=None # is related to shuffle\n",
    "start_index=None # is related to shuffle\n",
    "end_index=None # is related to shuffle\n",
    "n_features=21\n",
    "\n",
    "test_data=x_test_scale\n",
    "test_targets=y_test_scale\n",
    "\n",
    "dataRows = data.shape[0]\n",
    "validationStartRow = dataRows - NumSampsPerPing*50\n",
    "print(validationStartRow, ',',dataRows)\n",
    "\n",
    "train_data = data[:validationStartRow-1]\n",
    "#train_data.shape[0]\n",
    "train_targets = targets[:validationStartRow-1]\n",
    "#train_targets.shape[0]\n",
    "val_data = data[validationStartRow:]\n",
    "#val_data.shape[0], dataRows-validationStartRow\n",
    "val_targets = targets[validationStartRow:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#inputs=TimeseriesGenerator(data, targets,sequence_length, batch_size)\n",
    "train_inputs=TimeseriesGenerator(data=train_data,targets=train_targets,length=sequence_length,sampling_rate=sampling_rate,stride=sequence_stride,batch_size=batch_size)\n",
    "\n",
    "val_inputs = TimeseriesGenerator(data=val_data,targets=val_targets,length=sequence_length,sampling_rate=sampling_rate,stride=sequence_stride,batch_size=batch_size)\n",
    "\n",
    "#test_inputs=TimeseriesGenerator(test_data[0:40000], -1*test_targets[0:40000],sequence_length, batch_size)\n",
    "test_inputs=TimeseriesGenerator(data=test_data,targets=test_targets,length=sequence_length,sampling_rate=sampling_rate,stride=sequence_stride,batch_size=batch_size)\n",
    "\n",
    "#inputs=tf.keras.preprocessing.timeseries_dataset_from_array(data, targets, sequence_length, sequence_stride, sampling_rate, batch_size, shuffle)\n",
    "#print(type(inputs))\n",
    "\n",
    "#print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xArr = np.ndarray(shape=(900, batch_size, sequence_length, n_features))\n",
    "yArr = np.ndarray(shape=(900, batch_size))\n",
    "for i in range(900):    \n",
    "    [x0, y0]=train_inputs[i]\n",
    "    xArr[i] = x0\n",
    "    yArr[i] = y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xArr = xArr.reshape((900, 4278, 10, 21, 1))\n",
    "yArr = yArr.reshape((900, 4278, 1))\n",
    "x0.shape, y0.shape, type(x0), xArr.shape, yArr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first couple of samples to see if everything is lining up\n",
    "for i in range(100):\n",
    "\tx, y = train_inputs[i]\n",
    "\tprint('%s => %s' % (x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first couple of samples to see if everything everything is lining up\n",
    "for i in range(100):\n",
    "\tx, y = test_inputs[i]\n",
    "\tprint('%s => %s' % (x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Model's Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the model!\n",
    "model = 0\n",
    "#batch_size = 4278\n",
    "#sequence_length=10\n",
    "#n_features=21\n",
    "\n",
    "batch_size, sequence_length, n_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential(name='MLDOA_CNN')\n",
    "# https://androidkt.com/filters-kernel-size-input-shape-in-conv2d-layer/\n",
    "    # acceptable filter size f for something of size [32, 32, 3] is f*f*3 where f= 3, 5, 7 and so on...\n",
    "    # in our case, we have [batch_size, sequence_length, num_features] => [4278, 10, 21], so #filters = f*f*21, lets start w/ f = 3\n",
    "    # 3*3*21 = 9*21 = 189\n",
    "\n",
    "    # [sequence_length, num_features, 1] => [10, 21, 1], so #filters = f*f*1, say f = 7, num filters = 49\n",
    "kernelSize = 5\n",
    "\n",
    "\n",
    "cnn.add(Conv2D(filters=189, kernel_size=kernelSize, strides=1, activation='relu', input_shape=(batch_size, sequence_length,n_features)))\n",
    "#cnn.add(Conv2D(filters=49, kernel_size=kernelSize, strides=1, activation='relu', input_shape=(sequence_length,n_features, 1)))\n",
    "cnn.add(MaxPooling2D(pool_size=(kernelSize, kernelSize)))\n",
    "cnn.add(Flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With all the preproccessing done, it is time to define the model\n",
    "# Define the LSTM Model\n",
    "kernelSize = 2\n",
    "\n",
    "model= Sequential(name='MLDOASequential')\n",
    "model.add(TimeDistributed(Conv1D(filters=49, kernel_size=kernelSize, strides=1, activation='relu'), input_shape=(batch_size,sequence_length,n_features), name='TD_Conv1D'))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=(kernelSize)), name='TD_MaxPooling1D'))\n",
    "#model.add(TimeDistributed(Conv2D(filters=189, kernel_size=kernelSize, strides=1, activation='relu'), input_shape=(batch_size,sequence_length,n_features, 1), name='TD_Conv2D'))\n",
    "#model.add(TimeDistributed(MaxPooling2D(pool_size=(kernelSize, kernelSize)), name='TD_MaxPooling2D'))\n",
    "model.add(TimeDistributed(Flatten(), name='TD_Flatten'))\n",
    "#model.add(LSTM(units=21, name='LSTM1', activation='tanh', input_shape=(sequence_length,n_features), return_sequences=True))\n",
    "#model.add(LSTM(units=64, name='LSTM1', activation='tanh', batch_input_shape=(batch_size, sequence_length, n_features),input_shape=(sequence_length,n_features), return_sequences=True, stateful=False, bias_initializer='ones'))\n",
    "model.add(LSTM(units=64, name='LSTM1', activation='tanh', input_shape=(sequence_length,n_features), return_sequences=True, stateful=False, bias_initializer='ones'))\n",
    "#model.add(Dense(units=))\n",
    "model.add(Dense(units=100, name='Dense1', activation='relu'))\n",
    "model.add(Dropout(0.2, name='Dropout1'))\n",
    "\n",
    "model.add(LSTM(units=416, name='LSTM2', activation='tanh', return_sequences=True, stateful=False, bias_initializer='ones'))\n",
    "model.add(Dense(units=100, name='Dense2'))\n",
    "#model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dropout(0.2, name='Dropout2'))\n",
    "\n",
    "model.add(LSTM(units=512, name='LSTM3', activation='tanh', return_sequences=False, stateful=False, bias_initializer='ones'))\n",
    "model.add(Dense(units=100, name='Dense3'))\n",
    "model.add(Dropout(0.2, name='Dropout3'))\n",
    "\n",
    "#model.add(LSTM(units=84, name='LSTM3', activation='tanh', return_sequences=True))\n",
    "#model.add(Dense(units=100))\n",
    "#model.add(Dense(units=100, activation = 'linear'))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(LSTM(units=320, activation='tanh', return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(LSTM(units=168, name='LSTM4', activation='tanh'))\n",
    "#model.add(Dense(units=100))\n",
    "#model.add(Dense(units=100, activation = 'linear'))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(LSTM(units=480))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(units=1, name='DenseOutput')) #one output (namely: DOA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model using the Specified Hyperparmeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch learning rate scheduler for 10 epochs.\n",
    "def EpochScheduler10_2(epoch, lr):\n",
    "    if epoch == 0:\n",
    "        return 0.1\n",
    "    if epoch == 1:\n",
    "        return 0.1\n",
    "    elif epoch == 2:\n",
    "        return 0.1\n",
    "    elif epoch == 3:\n",
    "        return 0.05\n",
    "    elif epoch == 4:\n",
    "        return 0.05\n",
    "    elif epoch == 5:\n",
    "        return 0.05\n",
    "    elif epoch == 6:\n",
    "        return 0.025\n",
    "    elif epoch == 7:\n",
    "        return 0.025\n",
    "    elif epoch == 8:\n",
    "        return 0.025\n",
    "    elif epoch == 9:\n",
    "        return 0.01\n",
    "    elif epoch == 10:\n",
    "        return 0.01\n",
    "    elif epoch == 11:\n",
    "        return 0.01\n",
    "    else:\n",
    "        return 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch learning rate scheduler for 10 epochs.\n",
    "def EpochScheduler10(epoch, lr):\n",
    "    if epoch == 0:\n",
    "        return 0.01\n",
    "    if epoch == 1:\n",
    "        return 0.01\n",
    "    elif epoch == 2:\n",
    "        return 0.01\n",
    "    elif epoch == 3:\n",
    "        return 0.005\n",
    "    elif epoch == 4:\n",
    "        return 0.005\n",
    "    elif epoch == 5:\n",
    "        return 0.005\n",
    "    elif epoch == 6:\n",
    "        return 0.0025\n",
    "    elif epoch == 7:\n",
    "        return 0.0025\n",
    "    elif epoch == 8:\n",
    "        return 0.0025\n",
    "    elif epoch == 9:\n",
    "        return 0.001\n",
    "    elif epoch == 10:\n",
    "        return 0.001\n",
    "    elif epoch == 11:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now it is time to train the model\n",
    "opt=tf.keras.optimizers.Adam(learning_rate=0.01)#,decay=1e-5)\n",
    "#opt = tf.keras.optimizers.Adam()\n",
    "mMSE = tf.keras.metrics.MeanSquaredError()     # metric for Mean Squared Error\n",
    "mRMSE = tf.keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "model.compile(loss='mse', optimizer=opt)# metrics=[mMSE, mRMSE])\n",
    "\n",
    "# Allow for early stopping so that the model does not overfit the training dataset\n",
    "es= EarlyStopping(monitor='loss', mode='min',verbose=1,patience=10)\n",
    "\n",
    "# change learning rates per epoch\n",
    "lrCallback = tf.keras.callbacks.LearningRateScheduler(EpochScheduler10_2)\n",
    "\n",
    "# Model Checkpoint to save good runs\n",
    "#cp = ModelCheckpoint('model/', save_best_only=True)\n",
    "cp = ModelCheckpoint('model/', monitor='loss',save_best_only=True)\n",
    "\n",
    "t0=time.time()\n",
    "#history= model.fit(inputs,steps_per_epoch=4000,epochs=200,verbose=1, callbacks=[es])\n",
    "#history= model.fit(train_inputs, steps_per_epoch=4000, validation_data=val_inputs, epochs=100, callbacks=[cp])\n",
    "#history= model.fit(train_inputs, epochs=1, batch_size=None,callbacks=[cp, es])\n",
    "history= model.fit(x=xArr, y=yArr, steps_per_epoch = 30, epochs=100, callbacks=[cp])\n",
    "t1=time.time()\n",
    "print(\"The total run time to train was %.2f seconds\"%(t1-t0))\n",
    "\n",
    "# plot the loss function\n",
    "plt.figure()\n",
    "plt.semilogy(history.history['loss'])\n",
    "plt.xlabel('epoch'); plt.ylabel('loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Model, Predict the DOA's on the Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the model to predict on the testing dataset\n",
    "#for right now test_inputs has only the 40,001 samples\n",
    "predicted=model.predict(test_inputs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the predicted DOA's back into a csv file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ping |  Num Samp Num | PORT or STBD (0 or 1) |  TWTT |  Predicted DOA   <=======Output columns.\n",
    "OutputCSVdf=test.iloc[:,[0,1,2,3]]\n",
    "#OutputCSVdf= pd.DataFrame(test.iloc[:,[0,1,2,3]], columns=['PingNumber','SampNumber','PortStbd', 'SampleTime'])\n",
    "zerosArray= np.arange(sequence_length)*0\n",
    "predictedShifted=np.append(zerosArray, predicted)\n",
    "Predicteddf= pd.DataFrame(predictedShifted,columns=['PredictedDOA'])\n",
    "OutputCSVdf=OutputCSVdf.join(Predicteddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputCSVdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputCSVdf.to_csv(path_or_buf=\"D:\\\\OneDrive\\\\OneDrive - University of Massachusetts Dartmouth\\\\ECE457_Senior_Design_ECE5\\\\AIOutput_CSV_Files\\\\mb_predictions\\\\PredictedOutputTestMB3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[:,[0,1,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zerosArray= np.arange(sequence_length)*0\n",
    "zerosArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedShifted=np.append(zerosArray, predicted)\n",
    "predictedShifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted[0:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test[10:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predicted)\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_reshape= predicted.reshape(len(predicted),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## JUNK 1st 10 SAMPLES ARE JUNK ANYWAYS DON'T USE\n",
    "\n",
    "plt.figure(facecolor='white')\n",
    "plt.plot(-y_test[20:30], '.')\n",
    "plt.plot(predicted[20:30], '.')\n",
    "plt.title(\"Actual\")\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('DOA in degrees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(facecolor='white')\n",
    "plt.plot(y_test[0:40000])\n",
    "plt.plot(predicted[0:40000])\n",
    "plt.title(\"Predicted v Actual\")\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('DOA in degrees')\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the Actual Testing DOA's to the Predicted DOA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "\n",
    "\n",
    "plt.figure(facecolor='white')\n",
    "plt.plot(y_test[0:10], '.')\n",
    "plt.plot(predicted[0:10], '.')\n",
    "plt.title(\"Actual\")\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('DOA in degrees')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(facecolor='white')\n",
    "plt.plot(y_test[0:40000])\n",
    "plt.plot(predicted_reshape[0:40000])\n",
    "plt.title(\"Actual\")\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('DOA in degrees')\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(facecolor='white')\n",
    "plt.plot(y_test[0:40000])\n",
    "plt.title(\"Actual\")\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('DOA in degrees')\n",
    "plt.legend(['Actual'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(facecolor='white')\n",
    "plt.plot(predicted_reshape[0:40000], 'orange')\n",
    "plt.title(\"Actual\")\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('DOA in degrees')\n",
    "plt.legend(['Predicted'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.DataFrame(predicted, columns= ['DOA'])\n",
    "df2"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "674a9451ed4de84c8c52b62b7b95bc1f8984ddc9599597b49394de9958455427"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('keras_tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
