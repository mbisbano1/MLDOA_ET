{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#import np.array as array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################Gather Data from csv ##################################################\n",
    "# True DOA Data\n",
    "#df=pd.read_csv(r'C:\\Users\\rferr\\University of Massachusetts Dartmouth\\Michael R Bisbano - ECE457_Senior_Design\\CSV_Files\\StbdTesting_1404_002_FINAL.csv')\n",
    "df=pd.read_csv(r\"D:\\\\OneDrive\\\\OneDrive - University of Massachusetts Dartmouth\\\\ECE457_Senior_Design_ECE5\\\\CSV_Files\\\\StbdTesting_1404_002_FINAL.csv\")\n",
    "df_predicted=pd.read_csv(r\"D:\\\\OneDrive\\\\OneDrive - University of Massachusetts Dartmouth\\\\ECE457_Senior_Design_ECE5\\\\AIOutput_CSV_Files\\\\mb_predictions\\\\PredictedOutputTestMB7.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_DOA=-df.DOA\n",
    "true_DOA # True DOA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_DOA=df_predicted.PredictedDOA\n",
    "predicted_DOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garbage = 10 # Corresponding to number of bad samples in begininng of data set\n",
    "\n",
    "for i in range(0,garbage-1):\n",
    "    true_DOA[i]=0\n",
    "true_DOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Generate Arbitary Test Data ###########################################\n",
    "########### This part is completely optional, it just makes FAKE DATA ###################\n",
    "import random\n",
    "# This section can be adjusted based on when we get the predicted data\n",
    "# For now, its just test data that i made up\n",
    "test_length = true_DOA.size\n",
    "test_length ## how long the data array should be\n",
    "num_pings=1287\n",
    "\n",
    "test=[test_length]\n",
    "\n",
    "num_samples_per_ping = test_length/num_pings\n",
    "num_samples_per_ping=round(num_samples_per_ping)\n",
    "\n",
    "test_data = np.empty(test_length)\n",
    "for i in range(0, test_length):\n",
    "    test_data[i]=true_DOA[i]+random.randrange(-10,10)\n",
    "print(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length = true_DOA.size\n",
    "test_length ## how long the data array should be\n",
    "num_pings= abs(df.PingNum[0] - df.PingNum[test_length-1])\n",
    "num_samples_per_ping = test_length/num_pings\n",
    "num_samples_per_ping=round(num_samples_per_ping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_ping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This is an optional block to run. This only shows percent error for the entire DOA data set NOT for the average of each sample #########\n",
    "# It does show some interesting results if you care to see\n",
    "# \n",
    "\n",
    "##plt.cla\n",
    "#plt.figure(1)\n",
    "#plt.plot(test_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize Test Data and True DOA by average per sample\n",
    "# Then take the percent error of those points\n",
    "\n",
    "########### Organizing True DOA #############\n",
    "true_DOA_samp=np.zeros(shape=(num_pings-1,num_samples_per_ping))\n",
    "true_DOA_avg=np.zeros(shape=(num_samples_per_ping))\n",
    "summ=0\n",
    "# Organize by ping (ie array=ping by sample)\n",
    "for p in range(0,num_pings-1):\n",
    "    for s in range(0,num_samples_per_ping):\n",
    "        true_DOA_samp[p,s]=true_DOA[p*num_samples_per_ping + s]\n",
    "        \n",
    "# Average each sample (ie each column)\n",
    "for s in range(0,num_samples_per_ping):\n",
    "    for r in range(0,num_pings-1):\n",
    "        summ = summ + true_DOA_samp[p,s]\n",
    "    true_DOA_avg[s]=summ/num_samples_per_ping   \n",
    "    summ=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Organizing Test (Predicted) DOA #############\n",
    "test_DOA_samp=np.zeros(shape=(num_pings-1,num_samples_per_ping))\n",
    "test_DOA_avg=np.zeros(shape=(num_samples_per_ping))\n",
    "summ=0\n",
    "# Organize by ping (ie array=ping by sample)\n",
    "for p in range(0,num_pings-1):\n",
    "    for s in range(0,num_samples_per_ping):\n",
    "        test_DOA_samp[p,s]=predicted_DOA[p*num_samples_per_ping + s]\n",
    "        \n",
    "        \n",
    "# Average each sample (ie each column)\n",
    "for s in range(0,num_samples_per_ping):\n",
    "    for r in range(0,num_pings-1):\n",
    "        summ = summ + test_DOA_samp[p,s]\n",
    "    test_DOA_avg[s]=summ/num_samples_per_ping \n",
    "    summ=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_DOA_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_DOA_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_DOA_avg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(true_DOA_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DOA_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Percent Error Per Sample ###############\n",
    "percent_error = np.empty(num_samples_per_ping)\n",
    "for i in range(0, num_samples_per_ping):\n",
    "    percent_error[i] = abs((test_DOA_avg[i]-true_DOA_avg[i])/((true_DOA_avg[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Plotting Percent Error #####################\n",
    "# Plots the percent error of the average of each sample across each ping\n",
    "# ie a point of data represents the percent error for a sample for all pings\n",
    "# so if the x-axis=1 and y=2, there is a 200% perecent error for sample 1 averaged for all pings\n",
    "\n",
    "plt.cla\n",
    "plt.figure(1)\n",
    "plt.plot(percent_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anytyhing below here is for analysis purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, num_samples_per_ping): \n",
    "    if percent_error[i] > 2:\n",
    "        print(i)           # Prints every sample number with an average error above 200%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DOA_avg[1208]           # Predicted DOA at one of the above samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_DOA_avg[1208]          # True DOA at one of the above samples"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "674a9451ed4de84c8c52b62b7b95bc1f8984ddc9599597b49394de9958455427"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('keras_tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
